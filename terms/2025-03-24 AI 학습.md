자료: https://colab.research.google.com/drive/1iIhvEqckb-eDT5uhS92rnod2EVjSfj6K?usp=drive_link
공부한 거 : 
# ipynb
주피터 노트북 파일
코드와 문서 작성 함께할 수 있음

# MNLI
Multi-Genre Natural Understanding, 주어진 전제의 가설이 참인지 거짓인지, 무관한지 판단하는 방법
자연어 처리 task.? 중 하나
source: [자연어처리 관점](https://medium.com/@hugmanskj/%EC%9E%90%EC%97%B0%EC%96%B4-%EC%B2%98%EB%A6%AC-%EB%AC%B8%EC%A0%9C-%EA%B0%9C%EA%B4%80-understanding-%EA%B4%80%EC%A0%90-1-2-569911ddd1ca#:~:text=MNLI%20)
# tokenization
문자열을 여러 개의 조각, 즉 여러 개의 토큰으로 쪼개는 것. 한 단어가 토큰
 
# 가중치(w, weight)
각 입력 신호가 결과 출력에 미치는 중요도를 조절하는 매개변수
어떤 곳에 더 반영하고 덜 할지 결정함
-> 그 feature에 가중치가 높을 수록 더 많은 영향을 미침

source: https://inni-iii.tistory.com/42
# evaluate
파이썬 라이브러리
모델 비교 및 평가하고, 그것들의 수행 능력을 쉽고 표준화 시켜서 보고함
source: https://pypi.org/project/evaluate/
# unsupervised learning
데이터의 label이 정해지지않은, 정답이 없는 데이터셋을 통해 학습 수행
unsupervised data: 라벨링 안된 데이터
source: https://velog.io/@tjswodud/%EA%B8%B0%EA%B3%84%ED%95%99%EC%8A%B5-Supervised-Learning-Unsupervised-Learning
# imdb 데이터 구성
train, test, unsupervised data
train: 모델이 패턴을 배우는 과정에서 사용되는 데이터
test: 학습된 모델이 테스트를 진행할 때 사용하는 데이터
unsupervised: 라벨이 없는 데이터
# pre-trained
사전학습된 모델
파인튜닝하여 사용할 수 있음 -> transfer learning
### parameter
test_size: 0.0 ~ 1.0 default: none
사용될 테스트 데이터의 비중. test와 train데이터 모두 none일 경우 0.25로 자동설정됨

train_size: 0.0~1.0 default: none
사용될 트레이닝 데이터의 비중, 입력되지 않은 경우 테스트 데이터 외의 데이터들 모두 선택됨

두함수다 int형일 경우 비중이 아니라 입력된 정수 개의 데이터를 그용도로 사용함
source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

# BERT
구글에서 발표한 언어학습모델
사전 학습 모델임
# Transformer
입력데이터에서 중요한 정보를 추출하고 출력데이터를 생성하는 딥러닝 모델
self-attention mechanism 사용 ![[Pasted image 20250325151254.png]]
인코더 입력
원래 언어 문장을 토큰으로 만든 것의 인덱스들을 벡터 인덱스로 변환해서 인코더 입력을 만듦
## .keys()

dictionary의 키를 리턴...

ex

``` python

book1 = {

  'name': 'uyeong', # key: value 형태

  'year': 2002,

}

```

일 때 key: name, year
# trainer 라이브러리
https://sangwonyoon.tistory.com/61
# AutoModelForSequenceClassification
generic model class
automodel과 같은 역할인데 들어가는 모델이 다른듯 그 모델에 따라 configuration을 다르게 해야한다거나 

automodel from configure에 들어가는 모델이랑 for sequence  어쩌고 모델과 어떤 차이가 있는지 찾아보자
https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification


[`AutoModel`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#transformers.AutoModel "transformers.AutoModel") is a generic model class that will be instantiated as one of the base model classes of the library when created with the AutoModel.from_pretrained(pretrained_model_name_or_path) or the AutoModel.from_config(config) class methods.

# sequence classification
시퀀스 분류는 레이블이 지정된 시퀀스 데이터로 분류 모델을 학습시켜 보이지 않는 시퀀스의 클래스 레이블을 추론하는 방법
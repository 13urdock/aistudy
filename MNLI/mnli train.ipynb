{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuKOsPrFFAyA"
      },
      "outputs": [],
      "source": [
        "# load_dataset(\"nyu-mll/glue\", \"mnli\")\n",
        "# 로 dataset을 불러오기\n",
        "\n",
        "# 학습 때는 train split만 활용\n",
        "# Validation data가 필요한 경우, train split에서 가져오기\n",
        "\n",
        "#  trainer.train()를 통해 학습된 log가 남아있어야 함\n",
        "\n",
        "#  Dataset의 validation_matched에 대한 성능을 출력하고, 50%를 넘겨야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ38dROvFLeY"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets evaluate accelerate scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1uSA1GKshrD"
      },
      "source": [
        "# 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qLaf8Tq9Ganh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\aistudy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsNmI1enShhI"
      },
      "source": [
        "## AutoTokenizer\n",
        "다양한 tokenizer를 불러올 수 있는 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "Fk-BbzDfO1Qk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "392702\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"nyu-mll/glue\", \"mnli\")\n",
        "# dataset['train'][0]\n",
        "train_dataset = dataset['train']\n",
        "validation_matched = dataset['validation_matched']\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhSXKpf4ttS"
      },
      "source": [
        "## datasets - load_dataset\n",
        "hugging face 의 hub에서 dataset을 다운로드받을 수 있도록 만든 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "S_hWX07D41Wg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 9815/9815 [00:01<00:00, 9519.27 examples/s] \n"
          ]
        }
      ],
      "source": [
        "# 데이터 tokenize한 후 dataset_tokenized 에 저장\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def preprocess_function(data):\n",
        "  return tokenizer(\n",
        "      data[\"premise\"],\n",
        "      data[\"hypothesis\"],\n",
        "      truncation=True\n",
        ")\n",
        "\n",
        "dataset_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
        "validation_tokenized = validation_matched.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvJ4enBuSx0g"
      },
      "source": [
        "## tokenizer\n",
        "bert-base-cased 라는 이름의 tokenizer를 가져옴okenizer: bert-base-cased 라는 이름의 tokenizer를 가져옴\n",
        "-> bert model이 이해할 수 있는 형태로 변환해줌\n",
        "\n",
        "## preprocess_function\n",
        ": 토큰화된 데이터들의 결과\n",
        "\n",
        "## .map\n",
        "각 항목에 함수를 적용하는 메소드\n",
        "-> preprocess_function을 적용해 모두 토큰화함\n",
        "\n",
        "## batched\n",
        "데이터를 묶음으로 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TI2WkREdUqaV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(314161, 78541)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# train 데이터를 쪼개 training data와 validation data를 만들기\n",
        "dataset_split = dataset_tokenized.train_test_split(test_size=0.2)\n",
        "dataset_train, dataset_val = dataset_split['train'], dataset_split['test']\n",
        "\n",
        "len(dataset_train), len(dataset_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQSMPsqsYKW"
      },
      "source": [
        "# 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l7bwEx9YsIOl"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "config.hidden_size = 64\n",
        "config.intermediate_size = 64\n",
        "config.num_hidden_layers = 2\n",
        "config.num_attention_heads = 4\n",
        "config.num_labels = 3\n",
        "\n",
        "# 모델\n",
        "model = AutoModelForSequenceClassification.from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jnbzR5PmR8"
      },
      "source": [
        "# 학습코드\n",
        "\n",
        "## 학습인자 정의\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P90iwuvEPu9L"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# 학습인자 정의\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = 'training1',\n",
        "  num_train_epochs = 5,\n",
        "  per_device_train_batch_size = 128,\n",
        "  per_device_eval_batch_size = 128,\n",
        "  logging_strategy = \"epoch\", # epoch가 끝날 때마다 기록하기\n",
        "  do_train = True,\n",
        "  do_eval = True,\n",
        "  eval_strategy = \"epoch\",\n",
        "  save_strategy = \"epoch\",\n",
        "  learning_rate = 1e-3,\n",
        "  load_best_model_at_end = True # validation 가장 낮은 모델\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMVljTOzMzuE"
      },
      "source": [
        "[Training Arguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w0YDMMlPEAd"
      },
      "source": [
        "### batch_size\n",
        "데이터를 얼마나 잘라서 학습할건지 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8ez8wDVhPTdN"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\") # 정확도 평가하는 함수\n",
        "\n",
        "def compute_metrics(pred): # 성능 측정을 위한 측정 수치, pred: 예측정보와 정답을 모두 포함, 각각 predictions와 labels에 저장\n",
        "  predictions, labels = pred\n",
        "  predictions = np.argmax(predictions, axis=1) # 열을 따라 가장 큰 수의 인덱스들 반환\n",
        "  return accuracy.compute(predictions=predictions, references=labels) # 정확도\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6niPP9TBKPik"
      },
      "source": [
        "### evaluate 라이브러리\n",
        "모델 성능 평가하는 함수들의 모음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovXXnsD9Q9Ts"
      },
      "source": [
        "# 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "vA3z7jq8h0t1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\uyeon\\AppData\\Local\\Temp\\ipykernel_32184\\2620542733.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_val,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        "    # callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Hlm3HOF0FsUj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\aistudy\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='473' max='12275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  473/12275 1:24:08 < 35:08:24, 0.09 it/s, Epoch 0.19/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP9j9EkqYRTX"
      },
      "source": [
        "# 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqLP--jPqcTM"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate(validation_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcDel1uDqc9h"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
